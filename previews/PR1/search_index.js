var documenterSearchIndex = {"docs":
[{"location":"kmeans.html#K-Means","page":"K means","title":"K-Means","text":"","category":"section"},{"location":"kmeans.html","page":"K means","title":"K means","text":"The k means using Lloyd's algorithm can be generalized to manifolds, since its first step looks for the closest center of each data point, which can be done in terms of the geodesic distance. The second step of computing the mean within each cluster is generalized to computing the Riemannian center of mass[Karcher1977].","category":"page"},{"location":"kmeans.html","page":"K means","title":"K means","text":"Modules = [ManifoldML]\nPages = [\"kmeans.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"kmeans.html#ManifoldML.KMeansOptions","page":"K means","title":"ManifoldML.KMeansOptions","text":"KMeansOptions <: Options\n\nCollect the data necessary during computation of the k means clustering, i.e.\n\npoints::Vector{P} – the given data\ncenters::Vector{P} – the cluster centrs \nassignment::Vector{<:Int} – a vector the same length as points assigning each of them to a cluster\nstop::StoppingCriterion a stoppingCriterion\n\nHere P is a data type for points on the manifold the points (and centers) live on. This manifold is stored in the KMeansProblem.\n\nConstructor\n\nKMeansOptions(\n    points::Vector{P},\n    centers::Vector{P},\n    stop::StoppingCriterion=StoppingCriterion(100)\n)\n\nInitialize the options. The assignment is set to zero and initialized at the beginning of the algorithm.\n\n\n\n\n\n","category":"type"},{"location":"kmeans.html#ManifoldML.KMeansProblem","page":"K means","title":"ManifoldML.KMeansProblem","text":"KMeansProblem <: Problem\n\nStore the fixed data necessary for kmeans, i.e. only a Manifold M.\n\n\n\n\n\n","category":"type"},{"location":"kmeans.html#ManifoldML.kmeans-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,Array{P,1}}} where P","page":"K means","title":"ManifoldML.kmeans","text":"kmeans( M::Manifold, pts::Vector{P};\n    num_centers=5,\n    centers = pts[1:num_centers],\n    stop=StopAfterIteration(100),\n    kwargs...\n)\n\nCompute a simple k-means on a Riemannian manifold M for the points pts. The num_centers defaults to 5 and the initial centers centers are set to the first num_centers data items. The stopping criterion is set by default to 100 iterations.\n\nThe kwargs... can be used to initialize RecordOptions or DebugOptions decorators from Manopt.jl\n\nReturns the final KMeansOptions including the final assignment vector and the centers.\n\n\n\n\n\n","category":"method"},{"location":"kmeans.html#Literature","page":"K means","title":"Literature","text":"","category":"section"},{"location":"kmeans.html","page":"K means","title":"K means","text":"[Karcher1977]: Karcher, H.: Riemannian center of mass and mollifier smoothing, Communications on Pure and Applied Mathematics 30(5), 1977, pp. 509–541. doi: 10.1002/cpa.3160300502","category":"page"},{"location":"tangent-models.html#Tangent-space-models","page":"Tangent space models","title":"Tangent space models","text":"","category":"section"},{"location":"tangent-models.html","page":"Tangent space models","title":"Tangent space models","text":"One of the more popular methods of statistical modeling on manifolds is selecting a point p on the manifold mathcal M (for example Riemannian center of mass of data points), transforming all point to the tangent space T_pmathcal M at that point, and using traditional linear models in that tangent space. Tangent space Principal Component Analysis is a popular example of this kind of modeling. The example below demonstrates how to build and use such model.","category":"page"},{"location":"tangent-models.html","page":"Tangent space models","title":"Tangent space models","text":"using Manifolds, MultivariateStats\nM = Sphere(2)\npts = [project(M, randn(3)) for _ in 1:100]\nm = mean(M, pts)\nlogs = log.(Ref(M), Ref(m), pts)\nbasis = DefaultOrthonormalBasis()\ncoords = map(X -> get_coordinates(M, m, X, basis), logs)\ncoords_red = reduce(hcat, coords)\nz = zeros(manifold_dimension(M)))\nmodel = fit(PCA, coords_red; maxoutdim=1, mean=z)\nX = get_vector(M, m, reconstruct(model, [1.0]), basis)","category":"page"},{"location":"tangent-models.html","page":"Tangent space models","title":"Tangent space models","text":"The code performs the following steps:","category":"page"},{"location":"tangent-models.html","page":"Tangent space models","title":"Tangent space models","text":"Creating random data pts.\nComputing Riemannian center of mass m of pts.\nTransforming data to the tangent space at m using the logarithmic map.\nSelecting a basis of the tangent space at m in which the points will be represented.\nComputing coordinates of tangent vectors in said basis.\nTransforming a vector of vectors of coordinates into a matrix coords_red.\nFitting an ordinary PCA model to the matrix coords_red. Zero mean chosen, and the first principal direction is to be computed (the maxoutdim argument).\nThe first principal component is converted into a tangent vector X.","category":"page"},{"location":"tangent-models.html","page":"Tangent space models","title":"Tangent space models","text":"The same general procedure can be applied to other tangent space models by replacing steps 7 and 8.","category":"page"},{"location":"knn.html#K-Nearest-Neighbors","page":"K Nearest Neighbors","title":"K Nearest Neighbors","text":"","category":"section"},{"location":"knn.html","page":"K Nearest Neighbors","title":"K Nearest Neighbors","text":"K Nearest Neighbors (kNN) is a popular classification algorithm and a member of the family of instance-based learning methods.","category":"page"},{"location":"knn.html","page":"K Nearest Neighbors","title":"K Nearest Neighbors","text":"using Manifolds, ManifoldML, NearestNeighbors, StatsBase, Statistics\nM = SymmetricPositiveDefinite(3)\nN = 100\npts = [cov(randn(10, 3)) for _ in 1:N]\nys = [rand([1, 2]) for _ in 1:N]\ndist = ManifoldML.RiemannianDistance(M)\npoint_matrix = reduce(hcat, map(a -> reshape(a, 9), pts))\nballtree = BallTree(point_matrix, dist)\nfunction classify(tree, p, ys, k)\n    num_coords = prod(representation_size(balltree.metric.manifold))\n    idxs, dists = knn(tree, reshape(p, num_coords), k)\n    return mode(ys[idxs])\nend\nclassify(balltree, pts[2], ys, 3)","category":"page"},{"location":"knn.html","page":"K Nearest Neighbors","title":"K Nearest Neighbors","text":"The code performs the following steps:","category":"page"},{"location":"knn.html","page":"K Nearest Neighbors","title":"K Nearest Neighbors","text":"Creating random data pts on the manifold of symmetric positive definite matrices (i.e. covariance matrices).\nCreating random class labels ys.\nSelecting the distance dist to use in kNN.\nCollecting coordinates of points in a matrix point_matrix.\nCreating nearest neighbor search tree using the NearestNeighbors package.\nWriting a simple kNN classifier that reshapes the point p, performs a kNN search for k nearest neighbors and returns the most common label using the function mode from StatsBase.","category":"page"},{"location":"knn.html","page":"K Nearest Neighbors","title":"K Nearest Neighbors","text":"The same general procedure can be followed to build other distance-based classifiers.","category":"page"},{"location":"index.html#Machine-Learning-on-Manifolds","page":"Home","title":"Machine Learning on Manifolds","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"This package aims to provide machine learning tools on Riemannian manifolds based on ManifoldsBase.jl and Manopt.jl, as well as a few functions from Manifolds.jl. The package is inspired by PosDefManifoldML.jl doing Machine Learning on the Riemannian manifold of symmetric positive definite matrices.","category":"page"}]
}
